#!/usr/bin/env python3
"""
æ–‡æ¡£é‡å¤å†…å®¹æ£€æµ‹è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰Markdownæ–‡ä»¶
2. è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼ˆåŸºäºå†…å®¹ï¼‰
3. è¯†åˆ«é‡å¤çš„æ–‡ä»¶å†…å®¹
4. æ£€æµ‹ç›¸ä¼¼çš„å†…å®¹ï¼ˆåŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰
5. ç”Ÿæˆé‡å¤å†…å®¹æŠ¥å‘Š

ä½¿ç”¨åœºæ™¯ï¼š
- æ–‡æ¡£é‡æ•´åï¼Œæ£€æµ‹æ˜¯å¦æœ‰é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„å†…å®¹
- å¸®åŠ©è¯†åˆ«å¯ä»¥åˆå¹¶æˆ–åˆ é™¤çš„é‡å¤æ–‡æ¡£

ä½œè€…ï¼šAIåŠ©ç†
åˆ›å»ºæ—¥æœŸï¼š2025-12-31
"""

import os
import re
import sys
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Set, Optional
from collections import defaultdict
import difflib

class DocumentDuplicateChecker:
    """æ–‡æ¡£é‡å¤å†…å®¹æ£€æµ‹å™¨"""
    
    def __init__(self, docs_root: str):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        å‚æ•°ï¼š
            docs_root: æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„
        """
        self.docs_root = Path(docs_root).resolve()
        if not self.docs_root.exists():
            raise ValueError(f"æ–‡æ¡£æ ¹ç›®å½•ä¸å­˜åœ¨: {self.docs_root}")
        
        # æ–‡ä»¶å“ˆå¸Œç´¢å¼•ï¼šå“ˆå¸Œå€¼ -> æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        self.hash_index: Dict[str, List[Path]] = defaultdict(list)
        
        # æ–‡ä»¶å†…å®¹ç¼“å­˜ï¼šæ–‡ä»¶è·¯å¾„ -> å†…å®¹
        self.content_cache: Dict[Path, str] = {}
    
    def get_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼
        
        å‚æ•°ï¼š
            file_path: æ–‡ä»¶è·¯å¾„
        
        è¿”å›ï¼š
            MD5å“ˆå¸Œå€¼
        """
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = file_path.read_text(encoding='utf-8', errors='ignore')
        self.content_cache[file_path] = content
        
        # è®¡ç®—å“ˆå¸Œå€¼
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def build_hash_index(self) -> None:
        """
        æ„å»ºå“ˆå¸Œç´¢å¼•
        
        æ‰«ææ‰€æœ‰.mdæ–‡ä»¶ï¼Œè®¡ç®—å“ˆå¸Œå€¼å¹¶å»ºç«‹ç´¢å¼•
        """
        print(f"æ­£åœ¨æ„å»ºå“ˆå¸Œç´¢å¼•ï¼Œæ ¹ç›®å½•: {self.docs_root}")
        md_files = list(self.docs_root.rglob("*.md"))
        print(f"æ‰¾åˆ° {len(md_files)} ä¸ª.mdæ–‡ä»¶")
        
        for i, file_path in enumerate(md_files, 1):
            rel_path = file_path.relative_to(self.docs_root)
            if i % 50 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(md_files)}")
            
            try:
                file_hash = self.get_file_hash(file_path)
                self.hash_index[file_hash].append(rel_path)
            except Exception as e:
                print(f"  âš ï¸  è­¦å‘Šï¼šå¤„ç†æ–‡ä»¶ {rel_path} æ—¶å‡ºé”™: {e}")
        
        print(f"å“ˆå¸Œç´¢å¼•æ„å»ºå®Œæˆ")
    
    def find_exact_duplicates(self) -> Dict[str, List[Path]]:
        """
        æŸ¥æ‰¾å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶
        
        è¿”å›ï¼š
å­—å…¸            ï¼šå“ˆå¸Œå€¼ -> æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆè‡³å°‘2ä¸ªæ–‡ä»¶ï¼‰
        """
        duplicates = {}
        for file_hash, file_paths in self.hash_index.items():
            if len(file_paths) >= 2:
                duplicates[file_hash] = file_paths
        
        return duplicates
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        å‚æ•°ï¼š
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
        
        è¿”å›ï¼š
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0 ~ 1.0)
        """
        # ä½¿ç”¨difflibçš„SequenceMatcher
        matcher = difflib.SequenceMatcher(None, text1, text2)
        return matcher.ratio()
    
    def find_similar_documents(self, threshold: float = 0.8) -> List[Tuple[Path, Path, float]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡æ¡£
        
        å‚æ•°ï¼š
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤: 0.8)
        
        è¿”å›ï¼š
            åˆ—è¡¨ï¼š(æ–‡ä»¶1è·¯å¾„, æ–‡ä»¶2è·¯å¾„, ç›¸ä¼¼åº¦)
        """
        similar_pairs = []
        file_paths = list(self.content_cache.keys())
        
        print(f"æ­£åœ¨è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦ï¼Œå…± {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        # æ¯”è¾ƒæ¯å¯¹æ–‡ä»¶
        for i in range(len(file_paths)):
            path1 = file_paths[i]
            content1 = self.content_cache[path1]
            
            for j in range(i + 1, len(file_paths)):
                path2 = file_paths[j]
                content2 = self.content_cache[path2]
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(content1, content2)
                
                if similarity >= threshold:
                    rel_path1 = path1.relative_to(self.docs_root)
                    rel_path2 = path2.relative_to(self.docs_root)
                    similar_pairs.append((rel_path1, rel_path2, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        similar_pairs.sort(key=lambda x: x[2], reverse=True)
        
        return similar_pairs
    
    def generate_report(self, output_file: Optional[str] = None) -> str:
        """
        ç”Ÿæˆé‡å¤å†…å®¹æŠ¥å‘Š
        
        å‚æ•°ï¼š
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰)
        
        è¿”å›ï¼š
            æŠ¥å‘Šå†…å®¹
        """
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.append("=" * 60)
        report_lines.append("æ–‡æ¡£é‡å¤å†…å®¹æ£€æµ‹æŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ–‡æ¡£æ ¹ç›®å½•: {self.docs_root}")
        report_lines.append("=" * 60)
        report_lines.append("")
        
        # 1. å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶
        exact_duplicates = self.find_exact_duplicates()
        
        report_lines.append("1. å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶")
        report_lines.append("-" * 40)
        
        if exact_duplicates:
            report_lines.append(f"æ‰¾åˆ° {len(exact_duplicates)} ç»„å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶:")
            report_lines.append("")
            
            for i, (file_hash, file_paths) in enumerate(exact_duplicates.items(), 1):
                report_lines.append(f"ç¬¬ {i }ç»„ (å“ˆå¸Œ: {file_hash[:8]}...):")
                for file_path in file_paths:
                    report_lines.append(f"  - {file_path}")
                report_lines.append("")
        else:
            report_lines.append("âœ… æœªå‘ç°å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶")
            report_lines.append("")
        
        # 2. é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£
        similar_docs = self.find_similar_documents(threshold=0.7)
        
        report_lines.append("2. é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£ (ç›¸ä¼¼åº¦ â‰¥ 70%)")
        report_lines.append("-" * 40)
        
        if similar_docs:
            # æŒ‰ç›¸ä¼¼åº¦åˆ†ç»„
            high_similarity = [p for p in similar_docs if p[2] >= 0.9]
            medium_similarity = [p for p in similar_docs if 0.7 <= p[2] < 0.9]
            
            report_lines.append(f"é«˜åº¦ç›¸ä¼¼ (â‰¥90%): {len(high_similarity)} å¯¹")
            report_lines.append(f"ä¸­åº¦ç›¸ä¼¼ (70%-90%): {len(medium_similarity)} å¯¹")
            report_lines.append("")
            
            if high_similarity:
                report_lines.append("é«˜åº¦ç›¸ä¼¼æ–‡æ¡£å¯¹:")
                for path1, path2, similarity in high_similarity[:10]:  # æ˜¾ç¤ºå‰10å¯¹
                    report_lines.append(f"  - {path1} â†” {path2} ({similarity:.1%})")
                if len(high_similarity) > 10:
                    report_lines.append(f"  ... è¿˜æœ‰ {len(high_similarity) - 10} å¯¹æœªæ˜¾ç¤º")
                report_lines.append("")
            
            if medium_similarity:
                report_lines.append("ä¸­åº¦ç›¸ä¼¼æ–‡æ¡£å¯¹ (å‰10å¯¹):")
                for path1, path2, similarity in medium_similarity[:10]:
                    report_lines.append(f"  - {path1} â†” {path2} ({similarity:.1%})")
                if len(medium_similarity) > 10:
                    report_lines.append(f"  ... è¿˜æœ‰ {len(medium_similarity) - 10} å¯¹æœªæ˜¾ç¤º")
                report_lines.append("")
        else:
            report_lines.append("âœ… æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£")
            report_lines.append("")
        
        # 3. ç»Ÿè®¡
        report_lines.append("3. ç»Ÿè®¡ä¿¡æ¯")
        report_lines.append("-" * 40)
        
        total_files = len(self.content_cache)
        total_duplicates = sum(len(paths) for paths in exact_duplicates.values())
        unique_files = total_files - total_duplicates + len(exact_duplicates)
        
        report_lines.append(f"ğŸ“Š æ–‡ä»¶æ€»æ•°: {total_files}")
        report_lines.append(f"ğŸ“Š å”¯ä¸€æ–‡ä»¶æ•°: {unique_files}")
        report_lines.append(f"ğŸ“Š é‡å¤æ–‡ä»¶æ•°: {total_duplicates - len(exact_duplicates)}")
        report_lines.append(f"ğŸ“Š é‡å¤æ–‡ä»¶ç»„æ•°: {len(exact_duplicates)}")
        
        # é‡å¤æ–‡ä»¶å¤§å°ç»Ÿè®¡
        if exact_duplicates:
            total_wasted_space = 0
            for file_hash, file_paths in exact_duplicates.items():
                if file_paths:
                    # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å¤§å°
                    file_path = self.docs_root / file_paths[0]
                    if file_path.exists():
                        file_size = file_path.stat().st_size
                        wasted_space = file_size * (len(file_paths) - 1)
                        total_wasted_space += wasted_space
            
            report_lines.append(f"ğŸ“Š æ½œåœ¨æµªè´¹ç©ºé—´: {self._format(total_size_wasted_space)}")
        
        report_lines.append("")
        
        # 4. å»ºè®®æ“ä½œ
        report_lines.append("4. å»ºè®®æ“ä½œ")
        report_lines.append("-" * 40)
        
        if exact_duplicates or similar_docs:
            report_lines.append("å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå¤„ç†é‡å¤æ–‡æ¡£:")
            report_lines.append("")
            report_lines.append("1. å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶:")
            report_lines.append("   - ä¿ç•™ä¸€ä»½ï¼Œåˆ é™¤å…¶ä»–å‰¯æœ¬")
            report_lines.append("   - æ›´æ–°æ‰€æœ‰æŒ‡å‘è¢«åˆ é™¤æ–‡ä»¶çš„é“¾æ¥")
            report_lines.append("")
            report_lines.append("2. é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£ (â‰¥90%):")
            report_lines.append("   - æ¯”è¾ƒå†…å®¹å·®å¼‚ï¼Œå†³å®šæ˜¯å¦åˆå¹¶")
            report_lines.append("   - ä¿ç•™æ›´å®Œæ•´çš„ç‰ˆæœ¬ï¼Œåˆ é™¤å†—ä½™ç‰ˆæœ¬")
            report_lines.append("")
            report_lines.append("3. ä¸­åº¦ç›¸ä¼¼çš„æ–‡æ¡£ (70%-90%):")
            report_lines.append("   - è¯„ä¼°æ˜¯å¦éœ€è¦åŒæ—¶ä¿ç•™")
            report_lines.append("   - è€ƒè™‘é‡å†™æˆ–é‡æ„å†…å®¹")
            report_lines.append("")
            report_lines.append("ğŸ’¡ æç¤º: ä½¿ç”¨ docs_link_fix.py ä¿®å¤é“¾æ¥")
        else:
            report_lines.append("âœ… æ–‡æ¡£çŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«å¤„ç†")
        
        report_lines.append("")
        report_lines.append("=" * 60)
        
        report_content = "\n".join(report_lines)
        
        # å†™å…¥è¾“å‡ºæ–‡ä»¶
        if output_file:
            output_path = Path(output_file)
            output_path.write_text(report_content, encoding='utf-8')
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return report_content
    
    def _format_size(self, size_bytes: int) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        
        å‚æ•°ï¼š
            size_bytes: å­—èŠ‚æ•°
        
        è¿”å›ï¼š
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ£€æµ‹æ–‡æ¡£é‡å¤å†…å®¹   ')
 parser.add_argument('--docs-root', default='./docs',
                       help='æ–‡æ¡£æ ¹ç›®å½•è·¯å¾„ (é»˜è®¤: ./docs)')
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (å¯é€‰)')
    parser.add_argument('--threshold', type=float, default=0.8,
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤: 0.8)')
    parser.add_argument('--quick', action='store_true',
                       help='å¿«é€Ÿæ¨¡å¼ (ä»…æ£€æµ‹å®Œå…¨ç›¸åŒçš„é‡å¤)')
    
    args = parser.parse_args()
    
   :
 try        # åˆ›å»ºæ£€æµ‹å™¨
        checker = DocumentDuplicateChecker(args.docs_root)
        
        # æ„å»ºå“ˆå¸Œç´¢å¼•       
 checker.build_hash_index()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = checker.generate_report(args.output)
        
        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        print("\n" + "="*50)
        print("æ£€æµ‹å®Œæˆ!")
        print("="*50)
        
        # æ˜¾ç¤ºæ‘˜è¦
        exact_duplicates = checker.find_exact_duplicates()
        if exact_duplicates:
            print(f"âš ï¸  å‘ç° {len(exact_duplicates)} ç»„å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡ä»¶")
            for file_hash, file_paths in list(exact_duplicates.items())[:3]:  # æ˜¾ç¤ºå‰3ç»„
                print(f"  ç»„: {file_hash[:8]}... åŒ…å« {len(file_paths)} ä¸ªæ–‡ä»¶")
                for path in file_paths[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                    print(f"    - {path}")
                if len(file_paths) > 3:
                    print(f"    ... è¿˜æœ‰ {len(file_paths) - 3} ä¸ªæ–‡ä»¶")
                print()
        
        if not args.quick:
            similar_docs = checker.find_similar_documents(args.threshold)
            if similar_docs:
                print(f"âš ï¸  å‘ç° {len(similar_docs)} å¯¹ç›¸ä¼¼æ–‡æ¡£ (é˜ˆå€¼: {args.threshold})")
                for path1, path2, similarity in similar_docs[:5]:  # æ˜¾ç¤ºå‰5å¯¹
                    print(f"  - {path1} â†” {path2 ({}similarity:.1%})")
        
        if not exact_duplicates and (args.quick or not similar_docs):
            print("âœ… æœªå‘ç°é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡æ¡£")
        
        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²{'ä¿å­˜åˆ°: ' + args.output if args.output else 'åœ¨æ§åˆ¶å°æ˜¾ç¤º'}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())